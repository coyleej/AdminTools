\chapter{Slurm Adminstration} \label{ch:slurmAdministration}

\section{Slurm admin commands} \label{sec:slurmadmin}
These are the most common admin-specific commands \emph{in addition to the ones listed in section \ref{sec:queuecommands}}. (Those commands can be run with \texttt{sudo} to affect \emph{any} job.) For details, refer to \texttt{man <command>}.

\begin{itemize}
\item Setup, rebooting, and shutdown commands \\
\addcontentsline{lot}{table}{Slurm setup and management}
\begin{tabular}{l p{3.4in}}
  \texttt{scontrol takedown}                 & Orders switch to backup controller \\
  \texttt{scontrol reboot [ASAP] [Nodelist]} & Reboots nodes, see documentation \\
  \texttt{scontrol shutdown [slurmctld]}     & Saves the current slurm state, then shuts down the daemons \\
  \texttt{sacctmgr shutdown}                 & Shuts down the cluster \\
  \texttt{slurmd -C}                         & Displays the physical configuration of a node when run on that specific node \\
  \texttt{scontrol reconfigure}              & Makes running daemons re-read configuration files \\
\end{tabular}

\item Selected management and accounting commands \\
\addcontentsline{lot}{table}{Slurm management and accounting commands}
\begin{tabular}{l p{3.4in}}
  \texttt{sacct [options]} & Display accounting information for slurm jobs \\
  \texttt{sacctmgr}                      & View and modify slurm account info \\
  \texttt{sacctmgr add <entity> <specs>} & Add cluster, accounts, users; identical to \texttt{create} \\
  \texttt{sacctmgr list <entity> [specs]} & Displays information about the specified entity\\
  \texttt{sdiag}                         & Scheduling diagnostic tool \\
  \texttt{smd}                           & Failure management suport tool \\
  \texttt{sreport [options] [command]}   & Generates reports of job usage and cluster utilization \\
  \texttt{sstat}                         & Display various status information \\
  \texttt{sview}                         & Graphical user interface to view and modify slurm \\
\end{tabular}

%\item User management commands (beyond commands in the previous section)\\
%\addcontentsline{lot}{table}{Slurm user management}
%\begin{tabular}{l p{3.4in}}
%squeue -ho %A -t R | xargs -n 1 scontrol suspend
%squeue -o "%.18A %.18t" -u <username> | awk '{if ($2 =="S"){print $1}}' | xargs -n 1 scontrol resume
%squeue -ho %A -u $USER -t S | wc -l
%lsload |grep 'Hostname\|<partition>'
%\end{tabular}

\item Daemon commands: \texttt{slurmctld}, \texttt{slurmd}, and \texttt{slurmdbd} are the master/control, compute, and database daemons, respectively. They may need to be restarted if configuration files are modified (section \ref{sec:slurmconfig}). \\

\addcontentsline{lot}{table}{Slurm daemons}
\begin{tabular}{l p{3.8in}}
  \texttt{systemctld enable <daemon>}  & Enable to start on boot; will not start a stopped daemon \\
  \texttt{systemctld disable <daemon>} & Disable so that it will not start; will not stop a running daemon \\
  \texttt{systemctld start <daemon>}   & Starts daemon manually, does not enable the daemon \\
  \texttt{systemctld stop <daemon>}    & Only use \emph{if the cluster is fully idle} (you should default to \texttt{scontrol shutdown}): this stops daemon manually \emph{without saving the current slurm state} and does not disable the daemon \\
  \texttt{scontrol shutdown [slurmctld]} & Saves the current slurm state before stopping all slurm daemons (\texttt{slurmctld} option only shuts down the control daemons) \\
  \texttt{systemctld status <daemon>}  & Reports status of daemon \\
  \texttt{<daemon> -Dvvvv}             & Manually starts the daemon; ``D" runs in the foreground and ``v"s (can have 0 to 7 ``v"s) indicates desired verbosity \\
\end{tabular}

\end{itemize}

\section{Adjusting configuration files} \label{sec:slurmconfig}

If you modify the configuration files (\texttt{slurm.conf}, \texttt{slurmdbd.conf}, \texttt{cgroup.conf}), the change must be distributed to all nodes before applying the changes. The \texttt{distribute\_slurm\_conf.sh} script in \href{https://github.com/coyleej/MiniClusterTools}{MiniClusterTools repository} is one way to automate this process, as explained in section \ref{sec:propagate}.

\begin{enumerate}
  \item If you modify settings like Epilog, Prolog, SlurmctldLogFile, SlurmdLogFile, etc.), all you need to do is run \texttt{scontrol reconfigure} on the control node to force all slurm daemons to re-read the configuration files. The slurm controller (\texttt{slurmctld}) forwards the request to all other daemons (e.g.\ \texttt{slurmd}). Running jobs will continue execution. 
  \item Slurm daemons \emph{must be restarted} if any of these parameters are changed: AccountingStorageEnforce, AuthType, BackupAddr, BackupController, ControlAddr, ControlMach, PluginDir, StateSaveLocation, SlurmctldPort, SlurmdPort.
  \item Slurm daemons \emph{must be restarted} if nodes are added to or removed from the cluster.
\end{enumerate}

\texttt{ControlMachine} and \texttt{ControlAddr} are defunct in newer versions; use \texttt{SlurmctldHost} instead.

\subsection{Adding/removing nodes} \label{subsec:addnodes}

When adding/removing nodes, do the following:
\begin{enumerate}
  \item Stop \texttt{slurmctld}
  \item Add/remove nodes in \texttt{slurm.conf}
  \item Restart \texttt{slurmd} on all nodes
  \item Start \texttt{slurmctld}
\end{enumerate}

\noindent It is also possible to add nodes to \texttt{slurm.conf} with \texttt{state=FUTURE}. The nodes will not be seen by slurm commands in this state. Make them available by changing their state in the \texttt{slurm.conf} file and update the node state using \texttt{scontrol} rather than restarting the \texttt{slurmctld} daemon.

\section{Slurm plugins} \label{sec:slurmplugins}

Do not change the default Slurm plugin location in \texttt{slurm.conf}!

Default: \texttt{/usr/lib/x86\_64-linux-gnu/slurm-wlm}

\section{Reboot and shutdown nodes} \label{sec:slurmpowercycle}

\subsection{Reboot} \label{subsec:slurmreboot}

Nodes may need to be rebooted after firmware or kernel upgrades. Use the \texttt{RebootProgram} in \texttt{slurm.conf} to reboot nodes as they become idle. Be mindful of slurm downtime behavior (section \ref{subsec:slurmdowntime}).

\subsubsection{Controller reboot} \label{subsec:rebootSlurmControl}

Because of how our system is set up, you must complete the setup in section \ref{subsec:SetupBackupTakeover}. If you do not set this up, there is a high probability that you will kill all jobs when you reboot. (Note: this is dependent on the specific setup. On other systems the switch to the backup controller may be automatic and the following will not apply.)

The procedure depends on the controller being rebooted:

\begin{itemize}
  \item Primary controller:

    \begin{enumerate}
      \item Transfer the state to the backup controller, manually transfer the state, then stop \texttt{slurmctld}. This is automated in the \texttt{slurm\_transfer\_control.sh} script.

        If you are in the MiniClusterTools directory, you can call it with 

          \texttt{./transfer\_slurm\_control.sh}

        From other locations, you will need to specify the path

          \texttt{/<path>/<to>/transfer\_slurm\_control.sh}

        or use your aliased command.

      \item Follow the compute node reboot procedure or the normal shutdown procedure (\ref{subsec:slurmshutdown}).

      \item The admin user must log into the primary controller to start \texttt{slurmctld}. (\texttt{slurmctld} is disabled, and the start command is located in the admin user's \texttt{.bashrc} file.)

    \end{enumerate}

  \item Backup controller: 
  
    \begin{enumerate}
      \item \emph{Make sure that \emph{\texttt{slurmctld}} is running on the primary controller!} The primary controller will resume control as soon as \texttt{slurmctld} starts. In our setup, the admin \emph{must} log into the primary controller to start \texttt{slurmctld}. 

      \item Follow the compute node reboot procedure or the normal shutdown procedure (\ref{subsec:slurmshutdown}).

      \item The admin user must log into the backup controller to start \texttt{slurmctld}. (\texttt{slurmctld} is disabled, and the start command is located in the admin user's \texttt{.bashrc} file.)
    \end{enumerate}

\end{itemize}

\subsubsection{Compute node reboot} \label{subsec:rebootSlurmCompute}

Issue the appropriate command for your version of Slurm (you'll need \texttt{sudo}):

\begin{verbatim}
scontrol reboot [ASAP] [NodeList]                                              # 17.11.2
scontrol reboot [ASAP] [nextstate=<RESUME|DOWN>] [reason=<reason>] [NodeList]  # newer
\end{verbatim}

Explanation: \texttt{ASAP} prevents initiation of new jobs. Otherwise the system waits until it is idle to reboot and job scheduling is still allowed. The node state will be DRAIN (17.11.2) or REBOOT (newer) until rebooted or the reboot is cancelled. 

If you are rebooting 17.11.2 or older, you may need to manually resume the node with \texttt{scontrol} post-reboot. Newer versions of slurm include \texttt{nextstate}, which specifies the state of the node after reboot, and \texttt{reason}, which shows users the reason the node is unavailable.

To cancel a reboot, use one of the following

\begin{verbatim}
scontrol update NodeName=<nodename> State=RESUME   # slurm 17.11.2
scontrol cancel_reboot <nodelist>                  # newer versions, e.g. 18.08
\end{verbatim}

\subsection{Shutdown} \label{subsec:slurmshutdown}

Be mindful of slurm downtime behavior (section \ref{subsec:slurmdowntime}).

If you want to shut down the primary or backup controller without killing simulations, see \ref{subsec:rebootSlurmControl} to transfer control to the other machine before shutting anything down.

Shut down the slurm daemons with \texttt{scontrol shutdown [slurmctld]}. If the \texttt{slurmctld} option is used, only the control daemons will be shutdown. The benefit of \texttt{scontrol} over \texttt{systemctl} is that the former will save the current slurm state before shutting down the daemons.

Shut down the cluster with \texttt{sacctmgr shutdown}. 

\subsection{Slurm downtime behavior} \label{subsec:slurmdowntime}
Be mindful of your configured SlurmdTimeout and SlurmctldTimeout values. If the Slurm daemons are down for longer than the specified timeout (currently 5 minutes), nodes will be marked DOWN and their jobs killed. Either increase the timeout values during an upgrade or ensure that the compute node slurmd are not down for longer than SlurmdTimeout. 

\section{Backup and restore database} \label{sec:slurmDBbackup}

In order to backup the entire database to a different location (for disaster recovery or migration), the following files must be backed up. \href{https://wiki.fysik.dtu.dk/niflheim/Slurm_database#backup-and-restore-of-database}{(source)} Make a database mysqldump using this script /root/mysqlbackup (insert the correct root database password for PWD). 

\begin{verbatim}
#!/bin/sh
# MySQL Backup Script for All Databases
HOST=localhost
BACKUPFILE=/root/mysql_dump
USER=root
PWD='**********'
DUMP_ARGS="--opt --flush-logs --quote-names"
DATABASES="--all-databases"
/usr/bin/mysqldump --host=$HOST --user=$USER --password=$PWD $DUMP_ARGS \
     --result-file=$BACKUPFILE $DATABASES
\end{verbatim}

Write permission to \$BACKUPFILE is required.

Make regular database dumps, for example by a crontab job:
\texttt{30 7 * * * /root/mysqlbackup}

Restore of a database backup: The database contents must be loaded from the backup. To restore a MySQL database see for example \href{http://stackoverflow.com/questions/105776/how-do-i-restore-a-mysql-dump-file}{How do I restore a MySQL .dump file?}. As user root input the above created backup file:

\texttt{mysql -u root -p < /root/mysql\_dump}

\section{Upgrading slurm} \label{sec:slurmupgrade}

Almost every new major release of Slurm (e.g. 16.05.x to 17.02.x) involves changes to the state files with new data structures, new options, etc. Slurm permits upgrades between any two versions whose major release numbers differ by two or less (e.g. 16.05.x or 17.02.x to 17.11.x) without loss of jobs or other state information. State information from older versions will not be recognized and will be discarded, resulting in loss of all running and pending jobs. State files are not recognized when downgrading and will be discarded. Create backup copies of state files before proceeding to later recover the jobs.

\texttt{slurmdbd} must be the same or higher major release as \texttt{slurmctld}. When changing the version to a higher release number (e.g.\ from 16.05.x to 17.02.x) \emph{always} upgrade \texttt{slurmdbd} first. Database table changes may be required for the upgrade. If the database contains a large number of entries, \texttt{slurmdbd} may require an hour or two to update the database and will be unresponsive during this time.

\texttt{slurmctld} must be upgraded before or at the same time as \texttt{slurmd} on the compute nodes. It is recommended to update all daemons at the same time.

The \texttt{libslurm.so} version is increased every major release. Packages with slurm integration (e.g.\ MPI libraries) should be recompiled. Sometimes symlinking old \texttt{.so} name(s) to the new one(s) may work, but this is not guaranteed.

If you built your own version of Slurm plugins, they will likely need modification to support a new version of Slurm. It is common for plugins to add new functions and function arguments during major updates. See the RELEASE\_NOTES file for details.

The recommended upgrade order is as follows:

\begin{enumerate}
  \item Shutdown the slurmdbd daemon
  \item Dump the Slurm database using \texttt{mysqldump} in case of possible failure
  \item Increase \texttt{innodb\_buffer\_size} in \texttt{my.cnf} to 128M
  \item Upgrade the slurmdbd daemon
  \item Restart the slurmdbd daemon
  \item Increase SlurmdTimeout and SlurmctldTimeout values and \texttt{scontrol reconfigure} to take effect
  \item Shutdown the slurmctld daemon(s)
  \item Shutdown the slurmd daemons on the compute nodes
  \item Copy the contents of the configured StateSaveLocation directory in case of possible failure
  \item Upgrade the slurmctld and slurmd daemons
  \item Restart the slurmd daemons on the compute nodes
  \item Restart the slurmctld daemon(s)
  \item Validate proper operation
  \item Restore original SlurmdTimeout and SlurmctldTimeout, and then \texttt{scontrol reconfigure}
  \item Destroy backup copies of database and/or state files
\end{enumerate}

Note: It is possible to update the slurmd daemons on a node-by-node basis after the slurmctld daemon(s) are upgraded, but make sure their down time is below the SlurmdTimeout value.
